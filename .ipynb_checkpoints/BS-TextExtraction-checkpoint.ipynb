{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import re\n",
    "import string\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from spellchecker import SpellChecker\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# + jupyter={\"source_hidden\": true}\n",
    "## Reddit API Credentials\n",
    "#reddit = praw.Reddit(client_id='7_PY9asBHeVJxw',\n",
    "#                     client_secret='KL01wgTYZqwEDdPH-R8vNBqFYe4',\n",
    "#                     password='9S2a8a7hcr!',\n",
    "#                     user_agent='bravesoldier by /u/saayed',\n",
    "#                     username='saayed')\n",
    "#\n",
    "## Pull the subreddit of \n",
    "#subreddit = reddit.subreddit('leaves')\n",
    "#\n",
    "## Pulling top 1000 posts of leaves subreddit\n",
    "#leaves_subreddit = reddit.subreddit('leaves').top(limit=1000)\n",
    "#\n",
    "## Detail Information on the subreddit\n",
    "##print(subreddit.display_name, subreddit.title, subreddit.description, sep=\"\\n\")\n",
    "#\n",
    "## Create an empty dictionary to save data\n",
    "#dict = {'title': [],\n",
    "#        'body': [],\n",
    "#       }\n",
    "#\n",
    "## Storing the data in the empty dictionary\n",
    "#for submission in leaves_subreddit:\n",
    "#    dict['title'].append(submission.title)\n",
    "#    dict['body'].append(submission.selftext)\n",
    "#\n",
    "## Convert the data to pandas dataframe and apply date function\n",
    "#df = pd.DataFrame(dict)\n",
    "#df['raw'] = df['title'] + ' ' + df['body']\n",
    "#df.drop(['title', 'body'], axis=1, inplace=True)\n",
    "#\n",
    "## Print the first 5 rows of the data\n",
    "##df.head()\n",
    "#\n",
    "## Save it as CSV\n",
    "#df.to_csv('rleaves.csv', index=False)\n",
    "\n",
    "# +\n",
    "# Cleaning up the corpus\n",
    "def preprocessing(text):\n",
    "    \n",
    "    # lowercase the corpus \n",
    "    text = text.lower()\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\", ' ', str(text))\n",
    "    # removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # removing emoticons\n",
    "    text = re.sub('[^\\w\\s,]', ' ', str(text))\n",
    "    # removing zero-width space characters\n",
    "    text = re.sub('x200b', ' ', str(text))\n",
    "    # removing trailing whitespaces\n",
    "    text = ' '.join([token for token in text.split()])\n",
    "    # word tokenization\n",
    "    text = word_tokenize(text)\n",
    "    # additional removal of unnecessary words\n",
    "    stopwords_extra = ['im', 'ive', 'dont', 'didnt', 'doesnt', 'isnt', \n",
    "                       'couldnt', 'na', 'youre', 'cant', 'u', 'id', 'wasnt', \n",
    "                       'le', 'gon', 'pas', 'ill', 'youve', 'wont', 'havent', \n",
    "                       'wouldnt', '10184285', '179180', 'arent', 'youll', 'as', \n",
    "                       'oh', 'wan', 'av', 'p', 'ta', '10000']\n",
    "    text = [word for word in text if not word in stopwords_extra]\n",
    "    # join the words\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the local files\n",
    "rleaves = pd.read_csv('rleaves.csv', encoding='utf-8')\n",
    "\n",
    "# apply preprocessing function\n",
    "rleaves = pd.DataFrame(rleaves['raw'].apply(visual_processing))\n",
    "# -\n",
    "\n",
    "\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# DAY\n",
    "day = list(rleaves.raw.str.findall(r'\\d+\\s*day[s\\s]|\\s*day\\s*\\d+'))\n",
    "day = [int(item) for item in re.findall(r'\\d+', str(day))]\n",
    "day = pd.DataFrame.from_dict(Counter(day), orient='index').reset_index().rename(columns={'index':'day', 0:'count'})\n",
    "\n",
    "# Bin the days in to 7 day increment\n",
    "day_week = day.groupby(pd.cut(day['day'], np.arange(0, day['day'].max(), 7))).sum()\n",
    "day_week = day_week[day_week['count'] >= 1].drop(['day'], axis=1)\n",
    "\n",
    "# Visualizing the days\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "day_week.plot(kind=\"bar\", color='salmon', figsize=(20, 10))\n",
    "plt.xlabel(\"Period of Time\")\n",
    "plt.ylabel(\"Number of Appearance\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.title(\"Number of times 'Day' appeared in the r/leaves\")\n",
    "plt.show()\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# week\n",
    "week = list(rleaves.raw.str.findall(r'\\d+\\s*week[s\\s]|\\s*week\\s*\\d+'))\n",
    "week = [int(item) for item in re.findall(r'\\d+', str(week))]\n",
    "week = pd.DataFrame.from_dict(Counter(week), orient='index').rename(columns={0:'count'}).sort_index(ascending=True)\n",
    "\n",
    "# Visualizing the months\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "week.plot(kind='bar', color='salmon', figsize=(20, 10))\n",
    "plt.xlabel('Period of Time')\n",
    "plt.ylabel('Number of Appearance')\n",
    "plt.title('Number of times \"Week\" appeared in the r/leaves')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "rleaves[rleaves.raw.str.contains('\\s*month\\s*\\d+')].values.tolist()\n",
    "\n",
    "# +\n",
    "# MONTH\n",
    "month = list(rleaves.raw.str.findall(r'\\d+\\s*month[s\\s]|\\s*month\\s*\\d+'))\n",
    "month = [int(item) for item in re.findall(r'\\d+', str(month))]\n",
    "month = pd.DataFrame.from_dict(Counter(month), orient='index').rename(columns={0:'count'}).sort_index(ascending=True)\n",
    "\n",
    "month\n",
    "\n",
    "# +\n",
    "# MONTH\n",
    "month = rleaves[rleaves.raw.str.contains(r'\\d+ month|month \\d+')]\n",
    "month = list(month.raw.str.findall(r'\\d+ month|month \\d+'))\n",
    "month = [item[0].split(' ') for item in month]\n",
    "month = [' '.join(reversed(item)) for item in month if re.match(r'\\d', item[0])]\n",
    "month = pd.Series(month)\n",
    "month = month.value_counts().to_frame('Counts').reset_index(level=0)\n",
    "month.columns = ['Month', 'Counts']\n",
    "month['Month'] = month['Month'].str.replace(r'\\D', '').astype(int)\n",
    "month.set_index('Month', inplace=True)\n",
    "\n",
    "# Visualizing the months\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "month.plot(kind=\"bar\", color='salmon', figsize=(20, 10))\n",
    "plt.xlabel(\"Period of Time\")\n",
    "plt.ylabel(\"Number of Appearance\")\n",
    "plt.title(\"Number of times 'Month' appeared in the r/leaves\")\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# +\n",
    "# Cleaning up the corpus\n",
    "def preprocessing(text):\n",
    "    \n",
    "    # lowercase the corpus \n",
    "    text = text.lower()\n",
    "    # removing apostrophes\n",
    "    text = re.sub(\"'s\", ' ', str(text))\n",
    "    # removing punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    # removing emoticons\n",
    "    text = re.sub('[^\\w\\s,]', ' ', str(text))\n",
    "    # removing zero-width space characters\n",
    "    text = re.sub('x200b', ' ', str(text))\n",
    "    # removing trailing whitespaces\n",
    "    text = ' '.join([token for token in text.split()])\n",
    "    # word tokenization\n",
    "    text = word_tokenize(text)\n",
    "    # removing stopwords\n",
    "    stopwords_nltk = set(stopwords.words('english'))\n",
    "    text = [word for word in text if not word in stopwords_nltk]\n",
    "    # lemmatizing words\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    # additional removal of unnecessary words\n",
    "    stopwords_extra = ['im', 'ive', 'dont', 'didnt', 'doesnt', 'isnt', \n",
    "                       'couldnt', 'na', 'youre', 'cant', 'u', 'id', 'wasnt', \n",
    "                       'le', 'gon', 'pas', 'ill', 'youve', 'wont', 'havent', \n",
    "                       'wouldnt', '10184285', '179180', 'arent', 'youll', 'as', \n",
    "                       'oh', 'wan', 'av', 'p', 'ta']\n",
    "    text = [word for word in text if not word in stopwords_extra]\n",
    "    # join the words\n",
    "    text = ' '.join(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "# Load the local files\n",
    "rleaves = pd.read_csv('rleaves.csv', encoding='utf-8')\n",
    "\n",
    "# apply preprocessing function\n",
    "rleaves['raw'] = rleaves['raw'].apply(preprocessing)\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# WordCloud\n",
    "wordcloud_text = ' '.join(rleaves['raw'].tolist())\n",
    "\n",
    "def plot_cloud(wordcloud):\n",
    "    \n",
    "    plt.figure(figsize=(40, 30))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "\n",
    "    \n",
    "# WordCloud with Mask\n",
    "mask = np.array(Image.open('user.png'))\n",
    "wordcloud_user = WordCloud(width=3000, height=2000, random_state=1, background_color='rgba(255, 255, 255, 0)', mode='RGBA', colormap='tab10', collocations=False, mask=mask).generate(wordcloud_text)\n",
    "    \n",
    "#wordcloud_user.to_file(\"wordcloud_user_leaves.png\")\n",
    "plot_cloud(wordcloud_user)\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# Most common words\n",
    "top_words = Counter(' '.join(rleaves['raw']).split()).most_common(50)\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "top_words_barh = pd.DataFrame(top_words, columns=['word', 'count']).set_index('word').sort_values(by='count', ascending=True)\n",
    "top_words_barh.plot(kind='barh', color='salmon', figsize=(20,30), width=0.85)\n",
    "plt.show()\n",
    "# -\n",
    "\n",
    "processed_docs = processed_docs.str.replace(r'years|yrs', 'year')\n",
    "processed_docs = processed_docs.str.replace(r'\\smonths\\s', ' month ')\n",
    "\n",
    "# +\n",
    "# YEAR\n",
    "year = rleaves[rleaves.raw.str.contains(r'\\d+ year|year \\d+')]\n",
    "year = list(year.raw.str.findall(r'\\d+ year|year \\d+'))\n",
    "year = [item[0].split(' ') for item in year]\n",
    "year = [' '.join(reversed(item)) for item in year if re.match(r'\\d', item[0])]\n",
    "year = pd.Series(year)\n",
    "year = year.value_counts().to_frame('Counts').reset_index(level=0)\n",
    "year.columns = ['Year', 'Counts']\n",
    "year['Year'] = year['Year'].str.replace(r'\\D', '').astype(int)\n",
    "year.set_index('Year', inplace=True)\n",
    "\n",
    "# Visualizing the years\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "year.plot(kind='bar', color='salmon', figsize=(20, 10))\n",
    "plt.xlabel('Period of Time')\n",
    "plt.ylabel('Number of Appearance')\n",
    "plt.title('Number of times \"Year\" appeared in the r/leaves')\n",
    "plt.text(28, 28, 'Years could mean a) number of years smoked OR b) age', style='italic')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# Word Embedding\n",
    "corpus = rleaves['raw'].str.replace(r'\\d+', '').apply(word_tokenize).values.tolist()\n",
    "model_cbow = Word2Vec(corpus, min_count=9, window=3, sg=0, seed=1)\n",
    "model_skipgram = Word2Vec(corpus, min_count=9, window=3, sg=1, seed=1)\n",
    "\n",
    "print(model_cbow.most_similar('day'))\n",
    "print(model_skipgram.most_similar('day'))\n",
    "#model_cbow.save('model_cbow.bin')\n",
    "#new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
    "\n",
    "# +\n",
    "# TFIDF\n",
    "#tfidf = TfidfVectorizer()\n",
    "#bow_rep_tfidf = tfidf.fit_transform(processed_docs)\n",
    "#\n",
    "##IDF for all words in the vocabulary\n",
    "#print(\"IDF for all words in the vocabulary\\n\", tfidf.idf_)\n",
    "#print(\"_\"*10)\n",
    "#\n",
    "##All words in the vocabulary\n",
    "#print(\"All words in the vocabulary\\n\", tfidf.get_feature_names())\n",
    "#print(\"_\"*10)\n",
    "#\n",
    "##TFIDF representation of all documents in our corpus\n",
    "#print(\"TFIDF representation of all documents in our corpus\\n\", bow_rep_tfidf.toarray())\n",
    "#print(\"_\"*10)\n",
    "\n",
    "# + jupyter={\"outputs_hidden\": true}\n",
    "# Word Embedding\n",
    "corpus = processed_docs.str.replace(r'\\d+', '').apply(word_tokenize).values.tolist()\n",
    "model_cbow = Word2Vec(corpus, min_count=9, window=3, sg=0, seed=1)\n",
    "model_skipgram = Word2Vec(corpus, min_count=9, window=3, sg=1, seed=1)\n",
    "\n",
    "print(model_cbow.most_similar('age'))\n",
    "print(model_skipgram.most_similar('age'))\n",
    "#model_cbow.save('model_cbow.bin')\n",
    "#new_model_cbow = Word2Vec.load('model_cbow.bin')\n",
    "\n",
    "# +\n",
    "#VISUALIZATION\n",
    "\n",
    "#from gensim.models import Word2Vec, KeyedVectors\n",
    "#import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "#\n",
    "#import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from sklearn.manifold import TSNE\n",
    "#\n",
    "#model = KeyedVectors.load('model_cbow.bin')\n",
    "#\n",
    "#words_vocab = list(model.wv.vocab)\n",
    "#print(\"Size of Vocabulary: \", len(words_vocab))\n",
    "#print(\"=\"*30)\n",
    "#print(\"Few Words in Vocabulary\", words_vocab[:50])\n",
    "# -\n",
    "\n",
    "import spacy\n",
    "import textacy.ke\n",
    "from textacy import *\n",
    "\n",
    "en = textacy.load_spacy_lang('en_core_web_sm')\n",
    "\n",
    "# +\n",
    "text = ' '.join(rleaves['raw'])\n",
    "\n",
    "#with open('rleaves.txt', 'w') as output:\n",
    "#    output.write(text)\n",
    "# -\n",
    "\n",
    "doc = textacy.make_spacy_doc(text, lang=en)\n",
    "\n",
    "textacy.ke.textrank(doc, topn=5)\n",
    "\n",
    "textacy.ke.textrank(doc, topn=10)\n",
    "\n",
    "print('Textrank output: ', [kps for kps, weights in textacy.ke.textrank(doc, normalize='lemma', topn=10)])\n",
    "\n",
    "print('Textrank output: ', [kps for kps, weights in textacy.ke.textrank(doc, normalize='lemma', topn=10)])\n",
    "\n",
    "# + active=\"\"\n",
    "# SPACY CHAPTER 1\n",
    "#\n",
    "# from spacy.lang.en import English\n",
    "#\n",
    "# nlp = English()\n",
    "#\n",
    "# doc = nlp(rleaves['raw'][3])\n",
    "#\n",
    "# token = doc[1:3]\n",
    "#\n",
    "# print(token.text)\n",
    "#\n",
    "# # Process the text\n",
    "# doc = nlp(' '.join(rleaves['raw']))\n",
    "#\n",
    "# # Iterate over the tokens in the doc\n",
    "# for token in doc:\n",
    "#     # Check if the token resembles a number\n",
    "#     if token.like_num:\n",
    "#         # Get the next token in the document\n",
    "#         next_token = doc[token.i + 1]\n",
    "#         # Check if the next token's text equals \"%\"\n",
    "#         if next_token.text == \"year\":\n",
    "#             print(\"Percentage found:\", token.text)\n",
    "#\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# doc = nlp(rleaves['raw'][1])\n",
    "#\n",
    "# for token in doc:\n",
    "#     print(token.text, token.pos_, token.dep_, token.head.text)\n",
    "#     \n",
    "# for ent in doc.ents:\n",
    "#     print(ent.text, ent.label_)\n",
    "#\n",
    "# from spacy.matcher import Matcher\n",
    "#\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# matcher = Matcher(nlp.vocab)\n",
    "#\n",
    "# # pattern\n",
    "# pattern = [{'POS': 'ADJ'}, {'POS': 'NOUN'}, {'POS': 'NOUN', 'OP': '?'}]\n",
    "# matcher.add('DAY_PATTERN', None, pattern)\n",
    "#\n",
    "# doc = nlp(' '.join(rleaves['raw']))\n",
    "#\n",
    "# matches = matcher(doc)\n",
    "#\n",
    "# #print('Matches:', [doc[start:end].text for match_id, start, end in matches])\n",
    "#\n",
    "# for match_id, start, end in matches:\n",
    "#     print('Match found:', doc[start:end].text)\n",
    "#\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# doc = nlp(rleaves['raw'][2])\n",
    "#\n",
    "# for token in doc:\n",
    "#     # Get the token text, part-of-speech tag and dependency label\n",
    "#     token_text = token.text\n",
    "#     token_pos = token.pos_\n",
    "#     token_dep = token.dep_\n",
    "#     # This is for formatting only\n",
    "#     print(f\"{token_text:<12}{token_pos:<10}{token_dep:<10}\")\n",
    "#\n",
    "\n",
    "# +\n",
    "#print('SGRank output: ', [kps for kps, weights in textacy.ke.sgrank(doc, topn=10)])\n",
    "\n",
    "# +\n",
    "#terms = set([term for term, weight in textacy.ke.sgrank(doc)])\n",
    "#print(textacy.ke.utils.aggregate_term_variants(terms))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
